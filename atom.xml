<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>DJH&#39;s Blog</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-10-16T01:53:15.116Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>DJH</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>论文1</title>
    <link href="http://example.com/2021/10/16/%E8%AE%BA%E6%96%871/"/>
    <id>http://example.com/2021/10/16/%E8%AE%BA%E6%96%871/</id>
    <published>2021-10-16T01:49:58.000Z</published>
    <updated>2021-10-16T01:53:15.116Z</updated>
    
    <content type="html"><![CDATA[<h1 id="convolutional-LSTM-Network-A-Machine-Learning-Approach-for-Precipitation-Nowcasting-临近降水预报的机器学习方法"><a href="#convolutional-LSTM-Network-A-Machine-Learning-Approach-for-Precipitation-Nowcasting-临近降水预报的机器学习方法" class="headerlink" title="convolutional LSTM Network:A Machine Learning Approach for Precipitation Nowcasting(临近降水预报的机器学习方法)"></a>convolutional LSTM Network:A Machine Learning Approach for Precipitation Nowcasting(临近降水预报的机器学习方法)</h1><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h2><h3 id="降水预报问题的阐述"><a href="#降水预报问题的阐述" class="headerlink" title="降水预报问题的阐述"></a>降水预报问题的阐述</h3><p>降水预报的主要是利用雷达回波图去进行降水预报，大多是将过去观察得到的雷达回波图去预测将来一段时间内的雷达回波图，一般雷达回波图都是6min一张或者12min，预测1-6小时固定时间的图像序列。本质上是一个时空序列预测的问题。</p><h3 id="传统方法"><a href="#传统方法" class="headerlink" title="传统方法"></a>传统方法</h3><ol><li>NWP(longer-trem numerical weather prediction)</li><li>基于雷达的回波提取</li></ol><h3 id="最先进的方法与最新进展"><a href="#最先进的方法与最新进展" class="headerlink" title="最先进的方法与最新进展"></a>最先进的方法与最新进展</h3><p>最先进的方法：Real-time Optical flow by Variational methods for Echoes of Radar(ROVER)<br>最新进展：fully connected LSTM(FC-LSTM)，没有考虑空间的相关性。</p><h3 id="创新"><a href="#创新" class="headerlink" title="创新"></a>创新</h3><p>将FC-LSTM的思想扩展到在input-state、state-state的转换中都具有卷积结构的ConvLSTM。通过叠加多个ConvLSTM层并形成编码预测结构，建立一个端到端可训练的降水临近预测模型。</p><h2 id="2-Preliminaries"><a href="#2-Preliminaries" class="headerlink" title="2.Preliminaries"></a>2.Preliminaries</h2><h3 id="表达式"><a href="#表达式" class="headerlink" title="表达式"></a>表达式</h3><p><img src="https://img-blog.csdnimg.cn/70e7e930495345e89c6e7781cd5ca5c5.png" alt="在这里插入图片描述">复杂度：$O(M^KN^KP^K)$</p><h3 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h3><p>关于LSTM的文章：<br><a href="https://blog.csdn.net/qq_33431368/article/details/99859131">【Deep Learning】详细解读LSTM与GRU单元的各个公式和区别</a><br><a href="https://blog.csdn.net/qq_33431368/article/details/85288590">【TensorFlow实战笔记】通俗详述RNN理论,LSTM理论,以及LSTM对于PTB数据集进行实战</a></p><h3 id="FC-LSTM"><a href="#FC-LSTM" class="headerlink" title="FC-LSTM"></a>FC-LSTM</h3><p><img src="https://img-blog.csdnimg.cn/a8669ea83dab4b68b105c908fec2501d.png" alt="在这里插入图片描述"></p><ul><li>$i_t:$Input gate</li><li>$f_t:$Forget gate</li><li>$c_t:$New memory cell</li><li>$o_t:$output gate</li><li>$h_t:$hidden state</li></ul><h2 id="3-The-Model"><a href="#3-The-Model" class="headerlink" title="3.The Model"></a>3.The Model</h2><p>ConvLSTM和FC-LSTM的主要区别在于input-state,state-state的普通相乘改为卷积运算。全连接意味着相当于对于整体图像信息直接全部相乘等于一个值，这样无法对时空信息做一个特征提取的作用，而改为卷积即可。<br>对于卷积而言，如果卷积核大，捕捉到的偏向于更快的动作，如果卷积核小，捕捉到偏向于更慢的动作。</p><h3 id="Convolutional-LSTM"><a href="#Convolutional-LSTM" class="headerlink" title="Convolutional LSTM"></a>Convolutional LSTM</h3><p><img src="https://img-blog.csdnimg.cn/dc8d3719063b4e4aa5e0a2012a756ce6.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>红色部分就是inpute to state，紫色部分就是state to state.<br><img src="https://img-blog.csdnimg.cn/43486113467d457e9538a123bdb33528.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_18,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>为了保证输出的C和H与input X保持一样的维度，需要做卷积中的padding 操作</p><h3 id="Encoding-Forecasting-Structure"><a href="#Encoding-Forecasting-Structure" class="headerlink" title="Encoding-Forecasting Structure"></a>Encoding-Forecasting Structure</h3><p><img src="https://img-blog.csdnimg.cn/910dbda229fc4bd6a378489fe699dd87.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>对于论文中的时空序列预测问题，使用如图所示的结构，该结构由编码网络和预测网络组成。两个网络都由ConvLSTM层堆叠而成。为了保证预测结果和输入具有相同的维度，将预测网络的所有输出state连接起来，进行一个1*1的卷积。<br><strong>数学表达式</strong></p><p><img src="https://img-blog.csdnimg.cn/e7102a0d2236469597771abf971fd111.png" alt="在这里插入图片描述"></p><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4.Experiments"></a>4.Experiments</h2><h3 id="Moving-MNIST-Dataset"><a href="#Moving-MNIST-Dataset" class="headerlink" title="Moving-MNIST Dataset"></a>Moving-MNIST Dataset</h3><p>数据集为图中有两个数字的时空序列的移动。 大小为64乘以64， 整个序列为20，前十个为输入数据，后十个为预测数据。 也就是前十个为前十个时刻的数据，后十个为后十个时刻的数据.<br>所有的模型的均采用cross-entropy交叉熵作为损失函数，用的optimizer为RMSProp， 学习率为0.001并且有0.9的延迟率，在validation set上采用了 early-stopping整个操作<br><strong>时空序列训练的小trick</strong>：设置一个patchsize，把64乘64的矩阵转换为16乘16乘16的tensor。<br><img src="https://img-blog.csdnimg.cn/1092c546861e4432bd077984128326ad.png" alt="在这里插入图片描述"><br><strong>结论</strong></p><ol><li>Convlstm的效果比FC-lstm的效果好</li><li>更深的网络会更好，但是两层和三层差的不多</li><li>1乘1的state-to-state kernel size很难抓住时空移动的特征，所以效果差很多，所以更大的size更能够获取时空的联系</li></ol><h3 id="Radar-Echo-Dataset"><a href="#Radar-Echo-Dataset" class="headerlink" title="Radar Echo Dataset"></a>Radar Echo Dataset</h3><p>因为90%都是无雨的情况，为了防止模型将降水当作噪声或者负样本，采用降水量最高的97天作为训练数据。<br><img src="https://img-blog.csdnimg.cn/2005fe51b1794b51b60657d8ebbea96d.png" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2d4e244e47f44e6da483c37281600912.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_17,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">Convolutional LSTM Network--A Machine Learning Approach for Precipitation Nowcasting</summary>
    
    
    
    <category term="论文" scheme="http://example.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
    <category term="ConvLSTM" scheme="http://example.com/tags/ConvLSTM/"/>
    
    <category term="降水预报" scheme="http://example.com/tags/%E9%99%8D%E6%B0%B4%E9%A2%84%E6%8A%A5/"/>
    
    <category term="时空预测" scheme="http://example.com/tags/%E6%97%B6%E7%A9%BA%E9%A2%84%E6%B5%8B/"/>
    
  </entry>
  
  <entry>
    <title>论文2</title>
    <link href="http://example.com/2021/10/16/%E8%AE%BA%E6%96%872/"/>
    <id>http://example.com/2021/10/16/%E8%AE%BA%E6%96%872/</id>
    <published>2021-10-16T01:44:33.000Z</published>
    <updated>2021-10-16T08:57:25.286Z</updated>
    
    <content type="html"><![CDATA[<h1 id="论文：Self-Attention-ConvLSTM-for-Spatiotemporal-Prediction"><a href="#论文：Self-Attention-ConvLSTM-for-Spatiotemporal-Prediction" class="headerlink" title="论文：Self-Attention ConvLSTM for Spatiotemporal Prediction"></a>论文：Self-Attention ConvLSTM for Spatiotemporal Prediction</h1><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h2><p>   ConvLSTM在时空预测方面取得了不错的成绩，但通过堆叠的卷积层来捕获远程空间依赖性，有效的感受野远小于理论感受野。现有的改进方法TraiGRU工作方式类似于可变形卷积，学习到的偏移量仅提供了稀疏的空间依赖性。因此对于长距离的空间依赖的捕捉仍然是个问题<br>   而自注意力模块能够在单个网络层获取全局的空间上下文，所以本文引入了额外的记忆单元 M， 并且M可以通过门控机制来捕捉时间上长期的特征依赖。<br><strong>主要贡献</strong></p><p> 1.提出了一种新的ConvLSTM变体，称为SA-ConvLSTM，可以成功捕获长期的空间依赖性。<br> 2.设计了一个基于记忆的自我注意模块（memory-based self-attention module, SAM），以在预测过程中记忆全局时空依赖性。<br> 3.在MovingMNIST和KTH数据集上进行多帧预测，使用TexiBJ数据集进行交通流预测，以更少的参数和更高的效率在所有数据集中获得最佳结果。</p><h2 id="2-Methods"><a href="#2-Methods" class="headerlink" title="2.Methods"></a>2.Methods</h2><h3 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h3><p><img src="https://img-blog.csdnimg.cn/2dcddb902fca416e9e8093dd2d104f88.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_19,,t_70,g_se,x_16" alt="在这里插入图片描述"><br>$$H_t为当前时间步的特征图。query:Q_c=W_qH_t，key:K_h=W_kH_t，value:V_h=W_vH_t。$$<br>$$其中W_q,W_k,W_v都是1<em>1的卷积核，设原数据为C</em>H<em>W,C为通道数，则H_t为C</em>N，其中N=H*W.$$<br><strong>相似度得分计算公式:</strong><br>$$e=Q_h^TK_h∈R^{N×N}$$<br><strong>归一化：</strong><br>$$a_{i,j}=\frac{exp (e_{i,j})}{\sum^N_{k=1} exp(e_{i,k})} ,i,j∈{1,2,3…N}$$<br><strong>第i个位置的汇总特征是使用所有位置的加权总和来计算的：</strong><br>$$Z_i=\sum^N_{j=i}a_{i,j}(W_vH_{t;j})$$</p><h3 id="Self-Attention-Memory-Module"><a href="#Self-Attention-Memory-Module" class="headerlink" title="Self-Attention Memory Module"></a>Self-Attention Memory Module</h3><p><img src="https://img-blog.csdnimg.cn/120ec832b35243c0a93d9d2af1023c82.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>通过对Self-Attention基础模型加以改进，以捕捉时间和空间上的全局特征依赖，提出了Self-Attention Memory(SAM)模块，结构如上图所示。SAM模块接受两个输入：当前时间步的输入特征$H_t$和上一个时间步的记忆单元$M_{t-1}$,结构可分为三个部分：用以获取全局上下文信息的特征聚合(Feature Aggregation),以及记忆更新（Memory Updating）和输出（Output）。</p><p><strong>Feature Aggregation</strong><br><img src="https://img-blog.csdnimg.cn/ee02a7eff075478a83a4795a4149682a.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_19,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>通过两个Attention将当前时间步的输入特征$H_t$和上个时间步的记忆单元$M_{t-1}$，分别映射为$Z_h$和$Z_m$然后进行concat，经过1*1卷积得到后面的Z。</p><p><strong>Memory Updating</strong><br><img src="https://img-blog.csdnimg.cn/c6f946941b654bf182956b6db720125e.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_9,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>作者使用门控机制自适应的更新记忆单元M，聚合特征Z与$H_t$堆叠并经过一维卷积来计算更新门$i’_t$与更新值$g’_t$，遗忘门为$1-i’_t$<br><img src="https://img-blog.csdnimg.cn/e8049f8595834b97a21a11f34e226c79.png" alt="在这里插入图片描述"><br><strong>Output</strong><br><img src="https://img-blog.csdnimg.cn/8c01d7f332b0499dbbf1607e73aa03d9.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_12,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>最后，SAM通过更新后的记忆单元$M_{t-1}$与输出门$o’_t$按元素相乘得到最后的输出：</p><p><img src="https://img-blog.csdnimg.cn/9d819337b2664982b874b412b9139071.png" alt="在这里插入图片描述"></p><h3 id="Self-Attention-ConvLSTM"><a href="#Self-Attention-ConvLSTM" class="headerlink" title="Self-Attention ConvLSTM"></a>Self-Attention ConvLSTM</h3><p><img src="https://img-blog.csdnimg.cn/bc271f7c783541fca3e19fb2c028231d.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>将SAM模块嵌入到ConvLSTM中就得到了论文中所使用的SA-ConvLSTM<img src="https://img-blog.csdnimg.cn/2578831f2fea47138514022d16cf89df.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_14,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h3><p><img src="https://img-blog.csdnimg.cn/dc05035ea76f49c6a400104e3b8e9348.png" alt="在这里插入图片描述"></p><h3 id="对比实验"><a href="#对比实验" class="headerlink" title="对比实验"></a>对比实验</h3><p><img src="https://img-blog.csdnimg.cn/18e73b883ff54941b179b3ee951918f9.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/62833cb433244093b9598e873e9006f2.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p>]]></content>
    
    
    <summary type="html">Self-Attention ConvLSTM for Spatiotemporal Prediction</summary>
    
    
    
    <category term="论文" scheme="http://example.com/categories/%E8%AE%BA%E6%96%87/"/>
    
    
    <category term="ConvLSTM" scheme="http://example.com/tags/ConvLSTM/"/>
    
    <category term="时空预测" scheme="http://example.com/tags/%E6%97%B6%E7%A9%BA%E9%A2%84%E6%B5%8B/"/>
    
    <category term="Self-Attention" scheme="http://example.com/tags/Self-Attention/"/>
    
  </entry>
  
  <entry>
    <title>Pandas使用遇到的问题</title>
    <link href="http://example.com/2021/09/04/Pandas%E4%BD%BF%E7%94%A8%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://example.com/2021/09/04/Pandas%E4%BD%BF%E7%94%A8%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</id>
    <published>2021-09-04T01:55:59.000Z</published>
    <updated>2021-10-04T08:02:49.581Z</updated>
    
    <content type="html"><![CDATA[<h1 id="2121-9-3"><a href="#2121-9-3" class="headerlink" title="2121.9.3"></a>2121.9.3</h1><h2 id="note1"><a href="#note1" class="headerlink" title="note1"></a>note1</h2><p>合并数据，concat和merge</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data=pd.concat([data1,data2],ignore_index=True)</span><br></pre></td></tr></table></figure><p>纵向合并两DataFrame,ignore_index忽略数据原本的index，重新从0计数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data=pd.merge(data1,data2)</span><br></pre></td></tr></table></figure><p>横向合并两数据集</p><hr><h2 id="note2"><a href="#note2" class="headerlink" title="note2"></a>note2</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataFrame[&#x27;labe&#x27;]=DataFrame[&#x27;label&#x27;].apply(func)</span><br></pre></td></tr></table></figure><p>对一列标签的数据（即Series）数据进行操作,将里面的数据都进行func函数的调度</p><h2 id="note3"><a href="#note3" class="headerlink" title="note3"></a>note3</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataFrame[<span class="string">&#x27;label&#x27;</span>]=DataFrame[<span class="string">&#x27;labe&#x27;</span>].<span class="built_in">map</span>(<span class="type">Dict</span>)</span><br></pre></td></tr></table></figure><p>将label标签下的元素通过字典映射为新的元素</p><hr><h2 id="note4"><a href="#note4" class="headerlink" title="note4"></a>note4</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;label&#x27;</span>]=data[<span class="string">&#x27;label&#x27;</span>].fillna(x)</span><br></pre></td></tr></table></figure><p>data[‘label’]=data[‘label’].fillna(x)<br>对label标签下的空缺部分填充x</p><hr><h2 id="note5"><a href="#note5" class="headerlink" title="note5"></a>note5</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dict1=<span class="built_in">dict</span>(data[<span class="string">&#x27;label&#x27;</span>].value_counts())</span><br></pre></td></tr></table></figure><p>生成字典，key:label下的元素,value:相同元素的数量</p><hr><h2 id="note6"><a href="#note6" class="headerlink" title="note6"></a>note6</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.groupby(<span class="string">&#x27;label1&#x27;</span>).mean()</span><br></pre></td></tr></table></figure><p>将数据中label1列中的重复值合成一个值，然后把label1中的值当成索引来进行重新的数据分组</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.groupby(<span class="string">&#x27;label1&#x27;</span>)[<span class="string">&#x27;label2&#x27;</span>].mean()</span><br></pre></td></tr></table></figure><p>按label1列来进行分组，取label2列求平均值</p><hr><h2 id="note7"><a href="#note7" class="headerlink" title="note7"></a>note7</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.loc[]</span><br></pre></td></tr></table></figure><p>根据行列的标签值查询</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.loc[(data[<span class="string">&#x27;label1&#x27;</span>].apply(<span class="keyword">lambda</span> x:x <span class="keyword">in</span> <span class="built_in">list</span>)),<span class="string">&#x27;label2&#x27;</span>]=<span class="string">&#x27;x&#x27;</span></span><br></pre></td></tr></table></figure><p>查询出label1标签中的元素是否在list中，如果在，就将label2标签中的元素改为x</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data1.loc[(data.label1.isnull()),<span class="string">&#x27;lable1&#x27;</span>]=data2.label3</span><br></pre></td></tr></table></figure><p>将data1中label1标签空缺的部分，用data3中label3来填充</p><hr><h2 id="note8"><a href="#note8" class="headerlink" title="note8"></a>note8</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.iloc[]</span><br></pre></td></tr></table></figure><p>根据行列的索引位置查询</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data.iloc[:,:]</span><br></pre></td></tr></table></figure><p>整个数据，所有行所有列</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data.iloc[:,:].values[:,<span class="number">1</span>:]</span><br><span class="line">data.iloc[:,:].values[:,<span class="number">0</span>]</span><br></pre></td></tr></table></figure><p>提取所有行的第1列数据以后的所有数据<br>提取所有行的第0列数据</p><hr><h2 id="note9"><a href="#note9" class="headerlink" title="note9"></a>note9</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pd.get_dummies(data)</span><br></pre></td></tr></table></figure><p>对数据进行one-hot编码，DataFrame中数字部分不会进行one-hot编码，但Series会对数据进行one-hot编码</p><hr><h2 id="note10"><a href="#note10" class="headerlink" title="note10"></a>note10</h2><p>DataFrame中常用的方法</p><ul><li>data.head(5)    #查看前五行</li><li>data.tail(3)    #查看后三行</li><li>data.values    #查看数值</li><li>data.shape    #查看行,数列</li><li>data.filna(0)    #将空值填充0</li><li>data.replace(1,-1)    #将1替换成-1</li><li>data.isnull()    #查找数据中出现的空值</li><li>data.notll()    #非空值</li><li>data.dropna()    #删除空值</li><li>data.unique()    #查看唯一值</li><li>data.reset_index()    #修改、删除原有索引</li><li>data.columns    #查看数据的列名</li><li>data.index    #查看索引</li><li>data.sort_index()    #索引排序</li><li>data.sort_values()    #值排序</li></ul>]]></content>
    
    
    <summary type="html">pandas和dataframe遇到的问题</summary>
    
    
    
    <category term="Pandas" scheme="http://example.com/categories/Pandas/"/>
    
    <category term="DataFrame" scheme="http://example.com/categories/Pandas/DataFrame/"/>
    
    
    <category term="数据操作" scheme="http://example.com/tags/%E6%95%B0%E6%8D%AE%E6%93%8D%E4%BD%9C/"/>
    
  </entry>
  
  <entry>
    <title>机器学习-吴恩达</title>
    <link href="http://example.com/2021/08/30/%E5%88%9D%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    <id>http://example.com/2021/08/30/%E5%88%9D%E5%AD%A6%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</id>
    <published>2021-08-30T09:28:02.000Z</published>
    <updated>2021-12-07T05:02:33.578Z</updated>
    
    <content type="html"><![CDATA[<h1 id="初学机器学习"><a href="#初学机器学习" class="headerlink" title="初学机器学习"></a>初学机器学习</h1><h2 id="1监督学习-Supervised-learning"><a href="#1监督学习-Supervised-learning" class="headerlink" title="1监督学习(Supervised learning)"></a>1监督学习(Supervised learning)</h2><p>&emsp; &emsp;对于dataset中的每个样本，进行预测，并得到正确答案<br>&emsp; &emsp;回归问题（regression）：预测一组连续值的输出<br>&emsp; &emsp;分类问题（classification）：预测一组离散值的输出（0/1）</p><h2 id="2无监督学习"><a href="#2无监督学习" class="headerlink" title="2无监督学习"></a>2无监督学习</h2><p>&emsp; &emsp;聚类（clustering），将一组无特征值的数据进行划分</p><hr><h1 id="Linear-regression-with-one-variable-单变量线性回归"><a href="#Linear-regression-with-one-variable-单变量线性回归" class="headerlink" title="Linear regression with one variable(单变量线性回归)"></a>Linear regression with one variable(单变量线性回归)</h1><h2 id="3模型描述"><a href="#3模型描述" class="headerlink" title="3模型描述"></a>3模型描述</h2><p>&emsp; &emsp;假设一个函数(Hypothesis):这个函数将会预测输入的数据</p><h2 id="4代价函数（损失函数）"><a href="#4代价函数（损失函数）" class="headerlink" title="4代价函数（损失函数）"></a>4代价函数（损失函数）</h2><p>&emsp; &emsp;Hypothesis(假设函数):  $$ h_\Theta(x) = \Theta_0 +\Theta_1x $$<br>&emsp; &emsp;Parameters（参数）:  $$ \Theta_0 ,\Theta_1 $$<br>&emsp; &emsp;Cost Function（代价函数）: $$ J(\Theta_0,\Theta_1) =\frac{1}{2m} \sum_{i=1}^m(h_\Theta(x_i)-y_i)^2$$<br>&emsp; &emsp;Goal（目标）: $minimize_{\Theta_0,\Theta_1} J(\Theta_0,\Theta_1)$<br>平方差对于大多数问题，特别是回归问题都是一个合理的选择。当$J(\Theta_0,\Theta_1)$取值最小的时候，就是函数拟合最好的时候。<br>&emsp; &emsp;由于平方之后求导会多出两个常量2，所以代价函数乘以$\frac{1}{2}$正好可以消掉</p><h2 id="5梯度下降"><a href="#5梯度下降" class="headerlink" title="5梯度下降"></a>5梯度下降</h2><p>&emsp; &emsp;损失函数：$J(\Theta_0,\Theta_1)$</p><p>&emsp; &emsp;目标函数：$min_{\Theta_0,\Theta_1} J(\Theta_0,\Theta_1)$思路：</p><ul><li>随机初始化$\Theta_0,\Theta_1$</li><li>持续改变$\Theta_0,\Theta_1$，减少$J(\Theta_0,\Theta_1)$直到达到一个最小值<br>&emsp; &emsp;使用梯度下降的时候，每次计算都朝着下降速度最快的方向，但是不同的初始值可能导致最后走向不同的局部最低点。局部最低点可能不止一个。</li></ul><p><strong>梯度下降算法（Gradient descent algorithm）</strong><br>反复知道收敛（convergence）：<br><img src="https://img-blog.csdnimg.cn/15d79a638cee474d83748b2c6841847c.png" alt="在这里插入图片描述"><br><strong>注意：同步跟新</strong><br><img src="https://img-blog.csdnimg.cn/18da1d5fff4b4f72a965221e3ed92686.png" alt="在这里插入图片描述"><br><code>：=</code>为赋值符号<br>$\alpha$为学习率</p><h2 id="6梯度下降知识点总结"><a href="#6梯度下降知识点总结" class="headerlink" title="6梯度下降知识点总结"></a>6梯度下降知识点总结</h2><p>&emsp; &emsp;导数项决定斜率(slope)即下降方向始终朝向最小值。<br>&emsp; &emsp;如果当$\Theta$已经在局部最小，则此处斜率为0，$\Theta$将不会再改变。</p><p>&emsp; &emsp;如果学习率太小，会导致下降速度很慢，同时也可能陷入局部最优值(local optima)无法出来。<br>&emsp; &emsp;如果学习率太大，可能会跨过最小值，反复横跳，导致无法收敛(converge)甚至发散(diverge)<br>&emsp; &emsp;随着逼近局部最低点，斜率减小，此时每次更新的幅度也将随之自动变得越来越小，所以没必要改变$\alpha$</p><h2 id="7线性回归梯度下降"><a href="#7线性回归梯度下降" class="headerlink" title="7线性回归梯度下降"></a>7线性回归梯度下降</h2><p><img src="https://img-blog.csdnimg.cn/f4dfd5b37dc04f089b755cf4fb8fc0a7.png" alt="在这里插入图片描述"><br>&emsp; &emsp;分别求导:<br><img src="https://img-blog.csdnimg.cn/5031ddc2aabb42acacee9e780c317b27.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_9,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>&emsp; &emsp;凸函数(convex function)只有一个局部最优解<br>&emsp; &emsp;”Batch”梯度下降：意味着每一步下降都会使用整个训练集的样本</p><hr><h1 id="Linear-Algebra-review（线性代数回顾）"><a href="#Linear-Algebra-review（线性代数回顾）" class="headerlink" title="Linear Algebra review（线性代数回顾）"></a>Linear Algebra review（线性代数回顾）</h1><h2 id="8矩阵和向量"><a href="#8矩阵和向量" class="headerlink" title="8矩阵和向量"></a>8矩阵和向量</h2><p>&emsp; &emsp;矩阵的维数:行数×列数，(用大写字母表示矩阵)<br>&emsp; &emsp;$A_{ij}$=在矩阵里第i行第j列的元素</p><p>&emsp; &emsp;向量：n×1的矩阵，(用小写字母表示向量)<br>&emsp; &emsp;$y_i$=第i个元素</p><h2 id="9加法和标量乘法"><a href="#9加法和标量乘法" class="headerlink" title="9加法和标量乘法"></a>9加法和标量乘法</h2><p>&emsp; &emsp;矩阵的加法运算<br>&emsp; &emsp;矩阵的标量乘法运算</p><h2 id="10矩阵和向量的乘法"><a href="#10矩阵和向量的乘法" class="headerlink" title="10矩阵和向量的乘法"></a>10矩阵和向量的乘法</h2><p>&emsp; &emsp;将矩阵每一行的元素分别与向量的每一个元素相乘并加起来。<br><strong>注意</strong>：矩阵和向量相乘必须满足矩阵的列数与向量的维度相等</p><h2 id="11矩阵与矩阵相乘"><a href="#11矩阵与矩阵相乘" class="headerlink" title="11矩阵与矩阵相乘"></a>11矩阵与矩阵相乘</h2><p>&emsp; &emsp;m×n的矩阵和n×o的矩阵得到m×o的矩阵</p><h2 id="12矩阵乘法特征"><a href="#12矩阵乘法特征" class="headerlink" title="12矩阵乘法特征"></a>12矩阵乘法特征</h2><p>&emsp; &emsp;A×B不等于B×A，不满足交换律<br>&emsp; &emsp;A×B×C=A×(B×C)，满足结合率<br>&emsp; &emsp;单位矩阵(Identity Matrix):$I$or$I_{n×n}$</p><h2 id="13逆和转置-Inverse-transpose"><a href="#13逆和转置-Inverse-transpose" class="headerlink" title="13逆和转置(Inverse,transpose)"></a>13逆和转置(Inverse,transpose)</h2><p>&emsp; &emsp;$A$是一个n×n的方阵<br>&emsp; &emsp;$AA^{-1}=A^{-1}A$=$I$</p><p>&emsp; &emsp;<strong>注意</strong>:只有方阵才可能有逆矩阵，没有逆矩阵的矩阵称为奇异矩阵或退化矩阵(singular,degenerate)<br>&emsp; &emsp;$A$是一个m×n的矩阵，让$B=A^T$,那么$B$就是一个n×m的矩阵。$B_{ij}=A_{ji}$.</p><hr><h1 id="Linear-Regression-with-multiple-variables-多元线性回归"><a href="#Linear-Regression-with-multiple-variables-多元线性回归" class="headerlink" title="Linear Regression with multiple variables(多元线性回归)"></a>Linear Regression with multiple variables(多元线性回归)</h1><h2 id="14多特征"><a href="#14多特征" class="headerlink" title="14多特征"></a>14多特征</h2><p>&emsp; &emsp;$x^{(i)}$:第i个训练数据<br>&emsp; &emsp;$x^{(i)}_j$:第i个训练数据中的第j个特征值（第i行第j列）<br>&emsp; &emsp;假设函数：<br>$$h_\Theta(x)=\Theta_0+\Theta_1x_1+\Theta_2x_2+…+\Theta_nx_n$$<br>&emsp; &emsp;对于每个数据样例都有一个$x^{(i)}_0=1$<br>$$h_\Theta(x)=\Theta_0x_0+\Theta_1x_1+\Theta_2x_2+…+\Theta_nx_n$$</p><h2 id="15多元梯度下降法"><a href="#15多元梯度下降法" class="headerlink" title="15多元梯度下降法"></a>15多元梯度下降法</h2><p><img src="https://img-blog.csdnimg.cn/2cf84c79b8554946bc9035f4b2d69cd5.png" alt="在这里插入图片描述"><br>&emsp; &emsp;因为$x^{(i)}_0=1$,所以仍与之前的一元算法等效</p><h2 id="16梯度下降法演练1：特征缩放"><a href="#16梯度下降法演练1：特征缩放" class="headerlink" title="16梯度下降法演练1：特征缩放"></a>16梯度下降法演练1：特征缩放</h2><p>&emsp; &emsp;如果不同的特征值在相近的范围，这样梯度下降法就能更快的收敛。<br>&emsp; &emsp;通常我们在进行特征缩放的时候将范围大致限定在$-1\leq{x_i}\leq1$范围内。<br><strong>均值归一化（mean normalization）</strong><br>&emsp; &emsp;将$x_i$替换为$x_i-u_i$以使得特征值有接近于0的均值。但是不要将其应用到$x_0$上，因为$x_0$恒为1，不可能有为0到平均值。<br>通常使用如下式子：<br>$$x_i:= \frac{x_i-u_i}{range_i}$$<br>&emsp; &emsp;注：<br>&emsp; &emsp;$x_i$:第i个特征值<br>&emsp; &emsp;$u_i$:第i个特征值的均值<br>&emsp; &emsp;$range_i$第i个特征值的取值范围的长度（最大值减去最小值）<br>&emsp; &emsp;缩放不用特别精准，目的在于加快收敛速度</p><h2 id="17多元梯度下降法演练2-学习率"><a href="#17多元梯度下降法演练2-学习率" class="headerlink" title="17多元梯度下降法演练2-学习率"></a>17多元梯度下降法演练2-学习率</h2><p>&emsp; &emsp;通过绘制迭代次数与最小损失值($min_\theta J(\theta)$)的函数曲线，可以判断梯度下降算法是否正常运行。当曲线趋近于平坦的时候就代表差不多收敛了<br><img src="https://img-blog.csdnimg.cn/5b01b57d2aca48408d4964e09f99bbfb.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_11,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>&emsp; &emsp;同样也可以使用自动检测算法通过判断在一次迭代后$J(\theta)$是否小于某一阈值$\varepsilon$来判断是否收敛。但是阈值的取值通常难以确定，所以还是直接绘图观察比较好。</p><p>&emsp; &emsp;如果观察到$J(\theta)$曲线在上升或者在上下震荡，通常的办法就是降低学习率$\alpha$.</p><p>&emsp; &emsp;在选取$\alpha$的时候，我们从..0.001,0.01,0.1,1,…每隔10倍取值尝试，然后绘图观察，选取一个下降最快的$\alpha$</p><h2 id="18特征和多项式回归"><a href="#18特征和多项式回归" class="headerlink" title="18特征和多项式回归"></a>18特征和多项式回归</h2><p>&emsp; &emsp;多项式回归(polunomial regression)<br>$$h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3$$<br>&emsp; &emsp;在这种情况下，特征缩放就显得非常重要，这样才能将值的范围变得具有可比性。</p><h2 id="19正规方程"><a href="#19正规方程" class="headerlink" title="19正规方程"></a>19正规方程</h2><p>&emsp; &emsp;正规方程（normal equation）：直接求解 θ 最优值的方法即多元微分求极值。<br>$$\theta=(X^TX)^{-1}X^Ty$$<br>&emsp; &emsp;注解：<br>&emsp; &emsp;$m$个训练样本，$n$个特征。<br>&emsp; &emsp;X：所有特征的矩阵，其中$x_0=1$,所以其尺寸为：$m\times(n+1)$<br>&emsp; &emsp;y:所有样例的标签向量<br>&emsp; &emsp;使用正规方程进行计算的时候就不用进行特征缩放。</p><table><thead><tr><th>梯度下降</th><th>正规方程</th></tr></thead><tbody><tr><td>需要选择$\alpha$</td><td>不需要选择$\alpha$</td></tr><tr><td>需要很多次迭代</td><td>不需要迭代</td></tr><tr><td>当n很大的时候也能很好的工作</td><td>需要计算$(X^TX)^{-1}$</td></tr><tr><td>–</td><td>当n很大的时候会很慢，矩阵计算的代价以$O(n^3)增长$</td></tr></tbody></table><p>&emsp; &emsp;当特征数量小于10000的时候我们通常使用正规方程。</p><h2 id="20正规方程在矩阵不可逆情况下的解决方案"><a href="#20正规方程在矩阵不可逆情况下的解决方案" class="headerlink" title="20正规方程在矩阵不可逆情况下的解决方案"></a>20正规方程在矩阵不可逆情况下的解决方案</h2><p>&emsp; &emsp;一般矩阵不可逆的情况可能发生的原因：</p><ol><li>存在冗余数据(重复数据)，线性相关（linearly dependent）</li><li>特征太多了（m&lt;n）</li></ol><p>&emsp; &emsp;解决办法：</p><ol><li>合并重复特征，或者删除重复特征</li><li>删掉一些特征或者使用正规化</li></ol><hr><h1 id="logistic回归"><a href="#logistic回归" class="headerlink" title="logistic回归"></a>logistic回归</h1><h2 id="21假设陈述"><a href="#21假设陈述" class="headerlink" title="21假设陈述"></a>21假设陈述</h2><p>&emsp; &emsp;Sigmoid function和Logistic function是同义词，可以互换。<br>$$g(z)=\frac{1}{1+e^{(-z)}}$$<br><img src="https://img-blog.csdnimg.cn/d668ade30d1b4c9ea0f952ccfa1cefdf.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_14,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><strong>Logistic回归模型</strong><br>&emsp; &emsp;期望$$0\leq h_\theta(x) \leq1$$:<br>$$h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}=P(y=1|x;0)$$</p><h2 id="22决策界限"><a href="#22决策界限" class="headerlink" title="22决策界限"></a>22决策界限</h2><p>&emsp; &emsp;决策边界是假设函数的一个属性，有其参数决定，他不是数据集的属性。</p><h2 id="23代价函数"><a href="#23代价函数" class="headerlink" title="23代价函数"></a>23代价函数</h2><p><strong>线性回归代价函数</strong>:<br>$$Cost(h_\theta(x),y)=\frac{1}{2}(h_\theta(x)-y)^2$$</p><p><strong>Logistic回归代价函数</strong></p><p>$$Cost(h_\theta(x),y)=\begin{cases}-log(h_\theta(x)),&amp;if&amp;y=1 \<br>-log(1-h_\theta(x)),&amp;if&amp;y=0<br>\end{cases}$$<br><img src="https://img-blog.csdnimg.cn/c730ac75ce344b3d930ed9fe87b2d32e.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_11,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>&emsp; &emsp;$当y=1，h_\theta(x)=1时Cost=0,但是当h_\theta \to0时Cost \to\infty$</p><p><img src="https://img-blog.csdnimg.cn/6e3ee85aa87c4216ab5d872bbb6223df.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_11,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>&emsp; &emsp;y=0时同理。<br>&emsp; &emsp;上图显示出，当预测错误的时候，我们会用很大的代价值来惩罚学习算法</p><h2 id="24简化代价函数与梯度下降"><a href="#24简化代价函数与梯度下降" class="headerlink" title="24简化代价函数与梯度下降"></a>24简化代价函数与梯度下降</h2><p>&emsp; &emsp;有定义可知，总有y=0或y=1，所以可以对代价函数做如下简化：<br>$$Cost(h_\theta(x),y)=-ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))$$</p><p><strong>梯度下降</strong><br>$$J(\theta)=-\frac{1}{m}[\sum^m_{i=1}y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]$$<br>&emsp; &emsp;期望$min_\theta J(\theta)$,重复执行并同步更新$\theta_j$:<br>$$\theta_j:=\theta_j-\alpha \sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}$$<br>&emsp; &emsp;此处Logistic回归与线性回归看似很相似但区别在于$h_\theta(x)$不同：</p><ul><li>线性回归：$h_\theta(x)=\theta^TX$</li><li>Logistic回归:$h_\theta(x)=\frac{1}{1+e^{-\theta^TX}}$</li></ul><p>&emsp; &emsp;使用向量化的计算可保证同步更新。</p><h2 id="25高级优化"><a href="#25高级优化" class="headerlink" title="25高级优化"></a>25高级优化</h2><p>&emsp; &emsp;除梯度下降以外的优化算法还有：</p><ul><li>共轭梯度法(Conjugate gradient)</li><li>BFGS</li><li>L-BFGS</li></ul><p>&emsp; &emsp;这三种方法的优点在于：</p><ul><li>不需要手动选择学习率$\alpha$，每一次迭代他们都会使用线搜索算法找到最优的学习率。</li><li>收敛速度快于梯度下降</li></ul><p>&emsp; &emsp;缺点：更复杂。</p><h2 id="26多元分类：一对多"><a href="#26多元分类：一对多" class="headerlink" title="26多元分类：一对多"></a>26多元分类：一对多</h2><p>&emsp; &emsp;对于多元分类问题，我们可以将问题处理为多个二分类：一对多</p><p><img src="https://img-blog.csdnimg.cn/aa9b527fa133436c998044f20b917dc1.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>&emsp; &emsp;这样我们就得到了输入样例对每一类的概率，最终我们选取概率最高的一个作为它的分类结果。</p><hr><h1 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h1><h2 id="27过拟合问题"><a href="#27过拟合问题" class="headerlink" title="27过拟合问题"></a>27过拟合问题</h2><p><strong>过拟合</strong>：<br>&emsp; &emsp;模型过于简单，使得最后得出来的结果与实际结果偏差较大。<br><strong>欠拟合</strong>：<br>&emsp; &emsp;对于没有出现在训练集中的例子(泛化)可能表现很差，即泛化能力差。<br><img src="https://img-blog.csdnimg.cn/af5b8e4cfd89424ab44b87f1eee55150.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_15,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/4df040cf02bf4a62a9d70c9c8e9b9045.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_15,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><strong>解决过拟合</strong></p><p> 1.减少特征变量，两种办法<br>&emsp; &emsp;1.手工选选择需要保留的重要变量<br>&emsp; &emsp;2.模型选择法选择重要的要保留的变量<br>2.正则化。<br>&emsp; &emsp;1.保证所以的特征变量，但是通过调整特征变量前面参数来改变它的重要性。<br>&emsp; &emsp;2.这在有很多对结果有影响的特征变量的时候可能会比较有效。</p><h2 id="28代价函数"><a href="#28代价函数" class="headerlink" title="28代价函数"></a>28代价函数</h2><p>$$J(\theta)=\frac{1}{2m}[\sum^m_{i=1} (h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum^n_{j=1}\theta^2_j]$$<br>&emsp; &emsp;$\lambda$是正则化参数，用于控制两个不同目标之间的取舍，既要拟合目标函数，又要保持参数尽量 小，保持模型相对简单。<br>&emsp; &emsp;如果$\lambda$太大，将导致惩罚太大，最终欠拟合。<br>&emsp; &emsp;一般不对$x_0$进行正则化。</p><h2 id="29线性回归的正则化"><a href="#29线性回归的正则化" class="headerlink" title="29线性回归的正则化"></a>29线性回归的正则化</h2><p><strong>梯度下降</strong><br>&emsp; &emsp;重复执行：<br>$$\theta_0:=\theta_0- a\frac{1}{m}\sum^m_{i=1} (h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}$$<br>$$\theta_j:=\theta_j- a[\frac{1}{m}\sum^m_{i=1} (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j]$$<br>$$=\theta_j(1-\alpha \frac{\lambda}{m})-\alpha \frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} ， (j=1,…,n)$$<br>&emsp; &emsp;由于$\alpha、\lambda$都大于0，且$\alpha \frac{\lambda}{m}&lt;1$,所以本质上加了正则项以后的式子相对于没加之前的式子在每一次迭代的时候$\theta$乘了一个比1略小的数。<br><strong>正规方程</strong><br>$$\theta=(X^TX+\lambda\underbrace{\begin{bmatrix}<br>0 &amp; 0&amp;…&amp;0 \<br>0 &amp; 0&amp;…&amp;0\<br>…&amp;…&amp;…&amp;0\<br>0&amp;0&amp;0&amp;1<br>\end{bmatrix}}_{(n+1)\times(n+1)})^{-1}X^Ty$$<br>&emsp; &emsp;只要保证$\lambda$大于0，则加上正则项以后的矩阵必可逆。</p><h2 id="30Logistic回归的正则化"><a href="#30Logistic回归的正则化" class="headerlink" title="30Logistic回归的正则化"></a>30Logistic回归的正则化</h2><p>$$J(\theta)=-\frac{1}{m}[\sum^m_{i=1}y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum^n_{j=1}\theta^2_j$$<br><strong>梯度下降</strong><br>$$\theta_0:=\theta_0-\alpha\frac{1}{m} \sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}$$<br>$$\theta_j:=\theta_j-\alpha[\frac{1}{m} \sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j]$$<br>$$=\theta_j(1-\alpha \frac{\lambda}{m})-\alpha \frac{1}{m}\sum^m_{i=1}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} ， (j=1,…,n)$$</p><hr><h1 id="神经网络学习"><a href="#神经网络学习" class="headerlink" title="神经网络学习"></a>神经网络学习</h1><h2 id="31非线性假设"><a href="#31非线性假设" class="headerlink" title="31非线性假设"></a>31非线性假设</h2><p>&emsp; &emsp;当使用Logistic回归做非线性假设的时候，参数的数量会随着特征值数量n以$O(n^2)$的规模增长。当在计算图像数据的时候，参数的规模会超过百万及以上，此时使用Logistic回归计算会非常慢。所以我们选用神经网络来实现。</p><h2 id="32模型展示1"><a href="#32模型展示1" class="headerlink" title="32模型展示1"></a>32模型展示1</h2><p><img src="https://img-blog.csdnimg.cn/72c1de8914df4018a560eae34de6c388.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_14,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><img src="https://img-blog.csdnimg.cn/676cc00568ae4ad4a63fe77148e8decd.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_16,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>&emsp; &emsp;$x_0$被称为偏置单元或偏置神经元，取值为1.<br>&emsp; &emsp;$\theta$在此处一般被称之为权重。</p><p><img src="https://img-blog.csdnimg.cn/5e6ea2f9cf4c43d1981120093d6af521.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_16,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><hr><p>&emsp; &emsp;在神经元中，第一层被称为输入层，最后一层被称为输出层，中间的都被称为隐藏层。</p><p>$$ a_1^{(2)}=g(\theta_{10}^{(1)}x_0+\theta_{11}^{(1)}) $$<br>$$ a_1^{(2)}=g(\theta_{10}^{(1)}x_0+\theta_{11}^{(1)}x_1+\theta_{12}^{(1)}x_2+\theta_{13}^{(1)}x_3) $$</p><p>$$a_2^{(2)}=g(\theta_{20}^{(1)}x_0+\theta_{21}^{(1)}x_1+\theta_{22}^{(1)}x_2+\theta_{23}^{(1)}x_3)$$</p><p>$$a_3^{(2)}=g(\theta_{30}^{(1)}x_0+\theta_{31}^{(1)}x_1+\theta_{32}^{(1)}x_2+\theta_{33}^{(1)}x_3)$$</p><p>$$h_\theta(x)=g(\theta_{10}^{(2)}a_0^{(2)}+\theta_{10}^{(2)}a_1^{(2)}+\theta_{12}^{(2)}a_2^{(2)}+\theta_{13}^{(2)}a_3^{(2)})$$</p><p>&emsp; &emsp;每一个神经元都是一个激活函数。则整个神经网络的参数输了就是每一层参数数量的乘积。</p><h2 id="33模型展示2"><a href="#33模型展示2" class="headerlink" title="33模型展示2"></a>33模型展示2</h2><p>&emsp; &emsp;从输入层到计算出$h_\theta(x)$的过程称为前向传播。<br>&emsp; &emsp;神经网络所做的事情其实就和Logistic回归一样，只不过输入值变成了上一层网络的输出值。</p><hr><h1 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h1><h2 id="34代价函数"><a href="#34代价函数" class="headerlink" title="34代价函数"></a>34代价函数</h2><p><strong>逻辑回归</strong></p><p>$$J(\theta)=-\frac{1}{m}[\sum^m_{i=1}y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum^n_{j=1}\theta^2_j$$<br><strong>神经网络</strong></p><p>$$h_\theta(x)\in R^K(h_\theta(x)是一个K维向量),(h_\theta(x))_i是第i个输出$$</p><p><img src="https://img-blog.csdnimg.cn/9267adc944c34e1d8f4fd1c803718a9f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="35反向传播算法"><a href="#35反向传播算法" class="headerlink" title="35反向传播算法"></a>35反向传播算法</h2><p><strong>前向传播</strong><br>&emsp; &emsp;从输入层向前逐层运算后得出结果<br><img src="https://img-blog.csdnimg.cn/11b2f50894b0492c913efd274bc5c51a.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><strong>反向传播（backpropagation）</strong><br>&emsp; &emsp;直观上是算出真实值和神经网络的计算值之间的差。让代价函数最小化的算法</p><p>&emsp; &emsp;$\delta^{(l)}_j代表了第l层的第j个节点的误差$。<br>&emsp; &emsp;对每一个输出计算（第四层网络）：<br>&emsp; &emsp;$\delta^{(4)}_j=a^{(4)}_j-y_j，向量计算法：\delta^{(4)}=a^{(4)}-y$</p><hr><p>&emsp; &emsp;$\delta^{(3)}=(\theta^{(3)})^T\delta^{(4)}\cdot<br>g’(z^{(3)})$<br>&emsp; &emsp;$g’(z^{(3)})=a^{(3)}\cdot (1-a^{(3)})$</p><hr><p>&emsp; &emsp;$\delta^{(2)}=(\theta^{(2)})^T\delta^{(3)}\cdot<br>g’(z^{(2)})$<br>&emsp; &emsp;$g’(z^{(2)})=a^{(2)}\cdot (1-a^{(2)})$</p><hr><p>$$\frac{\partial}{\partial \theta^{(l)}_{ij}}J(\theta)=a^{(l)}_j\delta^{(l+1)}_i,（忽略\lambda或者\lambda=0）$$</p><h2 id="36理解反向传播"><a href="#36理解反向传播" class="headerlink" title="36理解反向传播"></a>36理解反向传播</h2><p><strong>前向传播</strong></p><p><img src="https://img-blog.csdnimg.cn/07054b3cbad249d2b1312529a4479442.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><strong>反向传播算法</strong><br>&emsp; &emsp;和前向传播非常相似，只是计算方向不同<br><img src="https://img-blog.csdnimg.cn/c483f9955f904139af76417fdb866788.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述">&emsp; &emsp;要注意，在反向传播过程中，并不更新偏置单元。</p><h2 id="37梯度检测"><a href="#37梯度检测" class="headerlink" title="37梯度检测"></a>37梯度检测</h2><p>&emsp; &emsp;梯度检测（gradient checking）用以检测前向传播和反向传播过程中出现的bug，主要用在反向传播上。<br><img src="https://img-blog.csdnimg.cn/4679259394064bedb2ae93522842b905.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>&emsp; &emsp;使用双侧差分而不是单侧差分，这样可以得到更精确的结果，$\varepsilon$不要取太小，以免遇到一些数值计算上的问题。<br>&emsp; &emsp;当我们把计算应用到参数向量上去的时候：<br><img src="https://img-blog.csdnimg.cn/0b10f2d2446f451f9862ad951bdc3bdd.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>&emsp; &emsp;通过比较梯度检测运算出来的梯度和反向传播运算出来的梯度是否相近，可以判断我们的训练结果是否正确</p><h2 id="38随机初始化"><a href="#38随机初始化" class="headerlink" title="38随机初始化"></a>38随机初始化</h2><p>&emsp; &emsp;当我们使用梯度下降或一些高级算法的时候，我们需要对θ选取一些初始值。<br>&emsp; &emsp;θ可以全部被初始化为0，但是在实际训练中，全部初始化为0起不了任何作用。所有的初始值相同会导致最终每个节点在梯度下降以后的结果相同，这意味着所有的节点都在计算相同的特征，这是一种高度荣誉的现象。最终的输出结果只计算了一个特征，这种情况阻止了网络去学习其他特征。<br>&emsp; &emsp;所以我们在初始化的时候应该选取接近于0的随机初始值。</p><h2 id="39组合到一起"><a href="#39组合到一起" class="headerlink" title="39组合到一起"></a>39组合到一起</h2><p>&emsp; &emsp;在进行神经网络训练之前，我们要选取一个合适的网络架构。<br>&emsp; &emsp;一般输入层的单元个数由你的数据集特征的维度所决定，输出层的单元个数取决于目标分类的个数。<br>&emsp; &emsp;对于应藏层，合理的默认值是一层应藏层，如果选择不止一层隐藏层的话将每层的节点个数设为相同，节点的个数越多越好，不过太多的话计算会很慢，一般设为输入层的几倍即可。</p><hr><h1 id="机器学习应用的一些建议"><a href="#机器学习应用的一些建议" class="headerlink" title="机器学习应用的一些建议"></a>机器学习应用的一些建议</h1><h2 id="40评估假设"><a href="#40评估假设" class="headerlink" title="40评估假设"></a>40评估假设</h2><p>&emsp; &emsp;将数据集划分成训练集和测试机，一般按照7：3的比例。<br><strong>线性回归的训练测试流程</strong><br>&emsp; &emsp;在训练集上学习参数$\theta$（最小化训练误差$J(\theta)$）<br>&emsp; &emsp;计算测试机误差：<br>$$J_{test}(\theta)=\frac{1}{2m_{test}}\sum_{i=1}^{m_{test}}(h_\theta(x_{test}^{(i)})-y_{test}^{(i)})^2 $$<br><strong>Logistic回归的训练测试流程</strong><br>&emsp; &emsp;对于Logistic回归也可以像线性回归一样在训练集上学习参数$\theta$,然后在测试集上计算误差$J(\theta)$.<br>&emsp; &emsp;还有一种评估标准叫做错误分类<br>$$err(h_\theta(x),y)=\begin{cases}1,&amp;if&amp;h_\theta(x)\geq0.5,y=0 &amp;or &amp; if&amp;h_\theta(x)&lt;0.5,y=1 \<br>0,&amp;otherwise&amp;<br>\end{cases}$$</p><p>$$Test_{error}=\frac{1}{m_{test}}\sum_{i=1}^{m_{test}}err(h_\theta(x_{test}^{(i)}),y_{test}^{(i)}) $$</p><h2 id="41模型选择和训练、验证、测试集"><a href="#41模型选择和训练、验证、测试集" class="headerlink" title="41模型选择和训练、验证、测试集"></a>41模型选择和训练、验证、测试集</h2><p>&emsp; &emsp;由于需要测试模型的泛化能力，如果之划分训练集和测试集，则在训练集上训练，用测试集找出复杂程度最适合的模型，接下来我们将无法进行泛化能力的检测。所以要在划分一个交叉验证集（cross validation）出来。<br>&emsp; &emsp;一般训练、验证、测试集经典划分为6:2:2.<br>&emsp; &emsp;接下来我们使用训练集训练模型，使用验证集选择模型，使用测试集来验证模型的泛化能力。</p><h2 id="42诊断偏差与方差"><a href="#42诊断偏差与方差" class="headerlink" title="42诊断偏差与方差"></a>42诊断偏差与方差</h2><p><img src="https://img-blog.csdnimg.cn/06082d7a8d8a4ffda9c5722cc4736577.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述">&emsp; &emsp;当模型偏差高的时候，训练误差和验证误差都很高，原因是模型简单。当模型方差高的时候，训练误差很低，但验证误差远大于训练误差，原因是模型过于复杂，泛化能力差。</p><h2 id="43正则化和偏差、方差"><a href="#43正则化和偏差、方差" class="headerlink" title="43正则化和偏差、方差"></a>43正则化和偏差、方差</h2><p>&emsp; &emsp;当正则项$\lambda$过大的时候会导致模型过于简单偏差过高欠拟合，反之过小的时候会导致模型过于复杂方差过高过拟合。<br><img src="https://img-blog.csdnimg.cn/eae4865a75e04f4ea2fb29cb0eb14df5.png" alt="在这里插入图片描述"></p><h2 id="44学习曲线"><a href="#44学习曲线" class="headerlink" title="44学习曲线"></a>44学习曲线</h2><p>&emsp; &emsp;随着训练样本的增加，训练误差会越来越大，想要拟合所有数据越来越难；而验证误差会逐渐减小，因为模型泛化能力增加。<br><img src="https://img-blog.csdnimg.cn/f65f8730aeaa44398f2d3461b2e950b4.png" alt="在这里插入图片描述"><br><strong>高偏差</strong><br>&emsp; &emsp;对于高偏差情况，验证误差和训练误差都很大，随着训练集的增加会逐渐趋于平台且很接近。对于高偏差情况继续增加训练样本并没有帮助。<br><img src="https://img-blog.csdnimg.cn/eed2a8b3472e412fbb1fc2cb33f8c450.png" alt="在这里插入图片描述"></p><p><strong>高方差</strong><br>&emsp; &emsp;对于高方差情况，一开始验证误差会很高，训练误差比较低，且两种误差之间简矩很大。但随着样本数量的增加，训练误差逐渐增大，验证误差逐渐减小，慢慢靠近。所以在高方差的情况下，增加训练样本是有帮助的<br><img src="https://img-blog.csdnimg.cn/cc4b7ea5046044de9043e4b3dcb8e7cc.png" alt="在这里插入图片描述"></p><h2 id="45决定接下来做什么"><a href="#45决定接下来做什么" class="headerlink" title="45决定接下来做什么"></a>45决定接下来做什么</h2><p><strong>线性回归</strong></p><p><img src="https://img-blog.csdnimg.cn/000ec83b690648e995d4369e9d2f3d65.png" alt="在这里插入图片描述"></p><p><strong>神经网络</strong><br>&emsp; &emsp;使用简单的神经网络，参数更少，计算量更小，但容易欠拟合。<br>&emsp; &emsp;使用复杂的神经网络，更多的参数，计算量更大，更容易过拟合。但是可以使用正则项来解决过拟合问题，通常复杂网络加正则项的方案比使用简单的神经网络更好。</p><hr><h1 id="机器学习系统设计"><a href="#机器学习系统设计" class="headerlink" title="机器学习系统设计"></a>机器学习系统设计</h1><h2 id="46误差分析"><a href="#46误差分析" class="headerlink" title="46误差分析"></a>46误差分析</h2><p>&emsp; &emsp;在系统设计上建议先使用一个简单粗暴的算法快速实现，然后进行误差分析，来了解你的模型主要是在哪些方面不足，这样能快速抓住后期工作的正确方向。<br>&emsp; &emsp;建议在交叉验证集上做误差分析，而不是在测试集上。使用数值评估方法很重要。</p><h2 id="47不对称性分类的误差评估"><a href="#47不对称性分类的误差评估" class="headerlink" title="47不对称性分类的误差评估"></a>47不对称性分类的误差评估</h2><p>&emsp; &emsp;当一类样本数量远低于另一类的时候，我们称其为偏斜类（skewed classes）。此时在使用精确度来评估模型将不再合适，而应使用查准率与召回率的度量标准。</p><p><img src="https://img-blog.csdnimg.cn/7f25df3f734b4d779ebf53bca2dc80e3.png" alt="在这里插入图片描述"></p><p> &emsp; &emsp;<strong>查准率</strong>：在我们预测为真的样本中，到底有多少真的为真样本。<br> $$查准率=\frac{真正例}{预测为真的例子}=\frac{TP}{TP+FP}$$<br> &emsp; &emsp;<strong>召回率</strong>：我们所预测为真的样本占总的正样本<br> $$召回率=\frac{真正例}{真实为真的例子}=\frac{TP}{TP+FN}$$</p><p>  &emsp; &emsp;使用查准率与召回率，即便是在有偏斜类的情况下，我们也能根据是否两项指标都很高来判断我们的算法是否很好。</p><h2 id="48-精确度和召回率的权衡"><a href="#48-精确度和召回率的权衡" class="headerlink" title="48 精确度和召回率的权衡"></a>48 精确度和召回率的权衡</h2><p> &emsp; &emsp;根据不同的情况，我们有时需要高查准率，有时又需要高召回率。对于不同的查准率和召回率，如何去决定哪一个模型更好，我们可以是用如下方法：<br> <strong>F1 Score</strong><br>$$F1Score=2\frac{PR}{P+R}$$<br>||Precision（P） |Recall（R）|Average|F1 Score|<br>|–|–|–|–|–|<br>|Algorithm1|0.5|0.4|0.45|0.444|<br>|Algorithm2|0.7|0.1|0.4|0.175|<br>|Algorithm3|0.02|1.|0.51|0.0392|<br> &emsp; &emsp;可以看出使用平均数并不是一个好的评估方法，而F1 Score的分子可以看出，只有在查准率和召回率都很高的时候得分才会很高。</p><h2 id="49机器学习数据"><a href="#49机器学习数据" class="headerlink" title="49机器学习数据"></a>49机器学习数据</h2><p> &emsp; &emsp;真正提高学习算法性能的方法是给予一个算法大量的训练数据，不同的算法可能会有细微的误差，但总的趋势都是随着数据量的增加从而性能提高。<br> <img src="https://img-blog.csdnimg.cn/b140858a61e64301a6666ffa83a004d0.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_14,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><hr><h1 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h1><h2 id="50优化目标"><a href="#50优化目标" class="headerlink" title="50优化目标"></a>50优化目标</h2><p><img src="https://img-blog.csdnimg.cn/5b55e147a09c4df8a3d47478116385bf.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><strong>Logistic回归</strong><br>$$min{<em>\theta}\frac{1}{m}[\sum^m</em>{i=1}y^{(i)}log(h_\theta(x^{(i)}))+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]+\frac{\lambda}{2m}\sum^n_{j=1}\theta^2_j$$<br><strong>支持向量机</strong><br>$$min{<em>\theta}C[\sum^m</em>{i=1}y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum^n_{j=1}\theta^2_j$$</p><p> &emsp; &emsp;区别1：支持向量机去掉了常数项$\frac{1}{m}$,但对最终结果并不影响。<br>  &emsp; &emsp;区别2：支持向量机使用$C$，而不是用$\lambda$<br>  <img src="https://img-blog.csdnimg.cn/aa9e73f229ac4d2fa44046fe78fd59d6.png" alt="在这里插入图片描述"><br>  注：n为特征数量，m为样本数量。<br>  <strong>假设函数形式</strong>：<br> $$h_\theta(x)=\begin{cases}1,&amp;if&amp;\theta^Tx \geq0 \0,&amp;otherwise&amp;<br>\end{cases}$$</p><h2 id="51直观上对大隔间的理解"><a href="#51直观上对大隔间的理解" class="headerlink" title="51直观上对大隔间的理解"></a>51直观上对大隔间的理解</h2><p>  &emsp; &emsp;$对于Logistic回归期望的是\theta^Tx\geq0或者\theta^Tx &lt;0即可做出分类。而支持向量机要求更高，它希望\theta^T\geq1或者\theta^x\leq-1。这就像是在支持向量机中构建了一个安全因子，一个安全间距$<br>  <img src="https://img-blog.csdnimg.cn/921f6bbde61e4051987dde4553fa43c1.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p> <strong>SVM决策边界</strong><br>  &emsp; &emsp;把优化问题看作是通过选择参数来使得第一项等于0.<br>  <img src="https://img-blog.csdnimg.cn/c2b9c64dc70b4ce78e8aa59b589b95e4.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><p>  <strong>线性可分例</strong><br><img src="https://img-blog.csdnimg.cn/df857a21d23a48f8be553039061aa7fc.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述">  &emsp; &emsp;黑色线条与蓝色线条的简矩称为支持向量机的简矩。相对于绿色和红色的线条，黑色的线会更加稳健，会尽量用大的间距去分离。所以有时支持向量机被称为大间距分类器。<br>  &emsp; &emsp;但支持向量机（黑线）要比大间距分类器（红线）更复杂。当C很大的时候，支持向量机会从黑线变成红线，但C不是很大的时支持向量机还是黑色这条线。如果数据不是线性可分的，支持向量机仍然可以正常工作。<br>  <img src="https://img-blog.csdnimg.cn/458a385358074d6ab68f775041478848.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="52大间隔分类器的数学原理。"><a href="#52大间隔分类器的数学原理。" class="headerlink" title="52大间隔分类器的数学原理。"></a>52大间隔分类器的数学原理。</h2><p>&emsp; &emsp;为简化证明，令:$\theta_0=0,n=2$<br> $$min_\theta\frac{1}{2}\sum_{j=1}n\theta^2_j=\frac{1}{2}(\theta^2_1+\theta^2_2)=\frac{1}{2}(\sqrt{\theta^2_1+\theta^2_2})^2=\frac{1}{2}||\theta||^2$$<br> $$\theta^Tx^{(i)}=P^{(i)}||\theta||=\theta_1x_1^{(i)}+\theta_2x_2^{(i)}$$<br> $$\theta^Tx^{(i)}\geq1\to p^{(i)}\cdot||\theta||\geq1 ,if:y^{(i)}=1$$<br> $$\theta^Tx^{(i)}\leq-1 \to p^{(i)}\cdot||\theta||\leq1 ,if:y^{(i)}=0$$<br><img src="https://img-blog.csdnimg.cn/7b7544d448ad4579a572064af98a194a.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_16,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述">&emsp; &emsp;$参数向量\theta实际上与决策边界成90°正交。因为令\theta_0=0,所以决策边界过0点$<br><img src="https://img-blog.csdnimg.cn/d5b9d9e1ee7d49c2947a1ee4211b5d9e.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述">&emsp; &emsp;$在第一种决策边界下可以看到p^{(1)}和p^{(2)}长度都很小，所以为了使p^{(1)}\cdot|\theta|\geq1,p^{(2)}\cdot|\theta|\leq-1,|\theta|必须很大。但是我们的目标函数的优化方向是减小|\theta|,所以这种决策边界不会被选择$<br><img src="https://img-blog.csdnimg.cn/5d7acc28c18f47859a328c9b66668ae2.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_18,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>&emsp; &emsp;同理，第二种决策边界的$|\theta|$计算出来会很小，所以这种决策边界会被选择。</p><h2 id="核函数1"><a href="#核函数1" class="headerlink" title="核函数1"></a>核函数1</h2><p><img src="https://img-blog.csdnimg.cn/79bc35425c014b0688ae799bf94b47e6.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述">&emsp; &emsp;我们手动选取3个landmark，然后根据这三个landmark计算出新的特征。<br>$$f_1=similarity(x,l^{(1)})=exp(-\frac{||x-l^{(1)}||^2}{2\sigma^2})=exp(-\frac{\sum^n_{j=1}(x_j-l_j^{(1)})^2}{2\sigma^2})$$<br>&emsp; &emsp;similarity函数我们称之为核函数，此处我们使用的是高斯核函数。为简化计算同样忽略$x_0$<br>&emsp; &emsp;如果$x\approx l^{(1)}$<br>$$f1\approx exp(-\frac{0^2}{2\sigma^2})\approx1$$<br>&emsp; &emsp;$如果x离l^{(1)}很远：$<br>$$f1\approx exp(-\frac{(large number)^2}{2\sigma^2})\approx0$$<br>&emsp; &emsp;同样的对$f_2,f_3$进行计算，我们就由$l^{(1)},l{(2)},l{(3)}$转化得到了三个新的特征$f_1,f_2,f_3$</p><p>&emsp; &emsp;$\sigma$是高斯核函数的一个参数，从下面的样例中可以看出$\sigma$的不同取值的影响。<br><img src="https://img-blog.csdnimg.cn/6ce5a41cba0e4236abe91dc13ff93989.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述">&emsp; &emsp;根据上面计算出的新特征进行计算，当样本落在红圈之内的时候预测为1，在外面的时候预测为0.<br><img src="https://img-blog.csdnimg.cn/4ede9fc524a24d0886122bc5c4e48467.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述">&emsp; &emsp;由此，我们就通过landmark和核函数训练出了非常复杂的非线性决策边界。</p><h2 id="53核函数2"><a href="#53核函数2" class="headerlink" title="53核函数2"></a>53核函数2</h2><p><strong>如何选取landmark</strong><br>&emsp; &emsp;我们直接将所以样本都取为landmark，然后使用这些landmark对每一个样本计算出新的特征向量$f^{(i)}$，其中$f^{(i)}<em>0=1$。接下来使用这些新的特征向量进行训练即可。<br><img src="https://img-blog.csdnimg.cn/4f465f6d92e846c99e546618feb75874.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><strong>训练</strong><br>$$min{<em>\theta}C[\sum^m</em>{i=1}y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum^m</em>{j=1}\theta^2_j$$<br>&emsp; &emsp;需要注意的一点是，在SVM实际实现中，最后一步$\sum^m_{j=1}\theta^2_j$会略有不同。实际实现中，会用$\theta^T$乘以某个矩阵M，这依赖于你采用的核函数，再乘$\theta$，即$\theta^TM\theta$。这其实是另一种略有区别的距离度量方法，这其实是$|\theta|^2$的缩放版本。这个数学上的优化使得SVM能更有效的运行在更大的训练集上，因为你将所有样板都作为了landmark，这样新的特征向量维度将会特别大，使用原来的式子计算将会非常大。<br>&emsp; &emsp;其实也可以将核函数的方法应用在Logistic回归等其他算法上，但是因为没有了专门为SVM设计的那样的优化，计算起来会非常慢。<br><strong>如何调参</strong><br>&emsp; &emsp;在进行SVM调参的时候，一个重要的优化目标是参数C，其作用和正则化参数$\lambda$相同。还有一个重要参数就是高斯核函数中的$\sigma$。<br><img src="https://img-blog.csdnimg.cn/ad9e81b1f5694813bb272e5e7761fce1.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="54使用SVM"><a href="#54使用SVM" class="headerlink" title="54使用SVM"></a>54使用SVM</h2><p>&emsp; &emsp;一般我们在使用SVM的时候选取核函数最常用的两种就是线性核函数和高斯核函数，没有核函数其实也算是线性核函数。<br>&emsp; &emsp;并不是所有你选用的similarity函数都能作为核函数。核函数要满足塞尔定理（Mercer‘s Theorem）。这是因为在实现SVM并进行优化的过程中，都是将注意力集中在可以满足塞尔定理的核函数上的。只有选取这样的核函数，你才能应用到现在大多数SVM软件包上去。<br>&emsp; &emsp;你还能选择的一些其他核函数：<br><img src="https://img-blog.csdnimg.cn/58b96200d9a94462950a2a15affc19d7.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>&emsp; &emsp;一般线性核函数性能会比较差。如果你准备做一些文本分类就可以选择字符串核函数。<br><strong>多分类</strong><br>&emsp; &emsp;一般的SVM软件包里已经实现了多分类函数，你只需调用就行了。<br>&emsp; &emsp;其实现思想还是one-vs-all。如果你需要分K类，你就需要训练K个SVM，然后分别对每一类做预测最终得到一个onehot的向量。<br><strong>Logistic回归VS SVM</strong><br>&emsp; &emsp;以n代表特征数，m代表样本数。<br>&emsp; &emsp;当n很大的时候（相对于m）：使用Logistic回归或者使用无核SVM（线性核函数）<br>&emsp; &emsp;当n很小，m比较适中的时候：使用高斯核函数的SVM。<br>&emsp; &emsp;当n很小，m很大的时候：添加更多特征，然后使用Logistic回归或者使用无核SVM（线性核函数）</p><hr><h1 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h1><h2 id="55-K-Means算法"><a href="#55-K-Means算法" class="headerlink" title="55 K-Means算法"></a>55 K-Means算法</h2><p><img src="https://img-blog.csdnimg.cn/5efd8be2b2bf459998bace7462e1eeb7.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>&emsp; &emsp;如果发现一个聚类中心没有簇点，则可以直接移除那个聚类中心，但这样分类的个数会减小。如果确实要保留原有的分类个数，可以考虑重新随机初始化中心点。但更常规的做法是直接移除。</p><h2 id="56优化目标"><a href="#56优化目标" class="headerlink" title="56优化目标"></a>56优化目标</h2><p><img src="https://img-blog.csdnimg.cn/c5df1e1336d447efb1675dee3714e0ce.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="57-随机初始化"><a href="#57-随机初始化" class="headerlink" title="57 随机初始化"></a>57 随机初始化</h2><p>&emsp; &emsp;直接随机选取K个样本作为初始的聚类中心是比较推荐的方法。<br>&emsp; &emsp;为了避免陷入局部最优解，要多次随机初始化，找到一个最好的结果。</p><h2 id="58选取聚类数量"><a href="#58选取聚类数量" class="headerlink" title="58选取聚类数量"></a>58选取聚类数量</h2><p>&emsp; &emsp;到目前为止，最好的方法还是进行数据可视化然后人工选取。<br><strong>手肘法</strong><br><img src="https://img-blog.csdnimg.cn/dbea296661684488a86b8e2e2a162f05.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>&emsp; &emsp;使用手肘法有时也并不能给出一个很好的结果。<br>&emsp; &emsp;有时我们使用K-Means是为了为下一个目标做准备。所以此时选取K值应该取决于选取哪一个分类数量会更好的服务于下一个目标</p><hr><h1 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h1><h2 id="59-目标1：数据压缩"><a href="#59-目标1：数据压缩" class="headerlink" title="59 目标1：数据压缩"></a>59 目标1：数据压缩</h2><p>&emsp; &emsp;降维是为了去除数据中高度冗余的特征，在节省空间的同时也加快了算法的运行速度。<br><img src="https://img-blog.csdnimg.cn/1da62d4221804e968e5ee53e623d8f17.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/ca99d0ef34f34dbc872e594147c10bad.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="60目标2：可视化"><a href="#60目标2：可视化" class="headerlink" title="60目标2：可视化"></a>60目标2：可视化</h2><p>&emsp; &emsp;降维也可以方便进行数据可视化</p><h2 id="61主成分分析问题规划1"><a href="#61主成分分析问题规划1" class="headerlink" title="61主成分分析问题规划1"></a>61主成分分析问题规划1</h2><p>&emsp; &emsp;对于降维问题，现在最流行最常用的一个算法叫做主成分分析方法（principal components analysis PCA）。<br>&emsp; &emsp;主成分分析要做的就是找到一个投影代价最小的平面对数据进行投影。在进行主成分分析之前，常规的做法是先进行数据均值归一化和特征规范化。<br><img src="https://img-blog.csdnimg.cn/470c2dbb7acf43feb7a04df050582158.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>&emsp; &emsp;PAC会选取红色那条投影代价最小的平面，而不是洋红色那条。<br>&emsp; &emsp;为找到这样的平面，首先需要在n维数据中找到一个向量$\mu^{(1)}$,其向量的方向为正或为负无所谓。<br>&emsp; &emsp;尽管PCA看起来和线性回归很像，但是却是完全不同的算法。PAC最小化的是正交距离，而线性回归最小化的是平方差距离。</p><h2 id="62主成分分析问题规划2"><a href="#62主成分分析问题规划2" class="headerlink" title="62主成分分析问题规划2"></a>62主成分分析问题规划2</h2><p><strong>数据预处理</strong></p><ol><li>均值归一化</li><li>特征规范化</li></ol><p><strong>PCA算法</strong><br>&emsp; &emsp;协方差矩阵计算：<br>$$\Sigma=\frac{1}{m}\sum^n_{i=1}(x^{(i)})({x^{(i)}})^T$$<br>&emsp; &emsp;协方差Sigma是一个n$\times$n的正定矩阵。<br>&emsp; &emsp;接下来我们要做的就是对协方差矩阵进行奇异值分解，得到一个矩阵U</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[U,S,V]=svd(Sigma)</span><br></pre></td></tr></table></figure><p>&emsp; &emsp;注：<br>&emsp; &emsp;S为一个对角阵，$U\cdot S\cdot V=Sigma$<br>&emsp; &emsp;取U前k列，构成一个$n\times k的降维矩阵U_{reduce}$,我们将使用这个矩阵来对我们的数据进行降维。<br>&emsp; &emsp;然后我们计算出一个k维向量z，这就是我们降维所得到的结果。<br><img src="https://img-blog.csdnimg.cn/e06ddf4fd50e48208278865280e749f9.png" alt="在这里插入图片描述"></p><h2 id="63主成分数量选择"><a href="#63主成分数量选择" class="headerlink" title="63主成分数量选择"></a>63主成分数量选择</h2><p>&emsp; &emsp;在PCA中我们将n维数据降维为k维，这个k是我们在PCA算法中需要调节的一个重要参数，称为主成分输了。<br>&emsp; &emsp;平均平方误差：<br>$$\frac{1}{m}\sum^m_{i=1}||x^{(i)}-x_{approx}^{(i)}||^2$$<br>&emsp; &emsp;总体方差：<br>$$\frac{1}{m}\sum^m_{i=1}||x^{(i)}||^2$$<br>&emsp; &emsp;典型的k值选择需要满足99%的方差将会被保留，常用的比例还有95%、90%等等。<br>$$\frac{\frac{1}{m}\sum^m_{i=1}||x^{(i)}-x_{approx}^{(i)}||^2}{\frac{1}{m}\sum^m_{i=1}||x^{(i)}||^2}\leq0.01$$<br>&emsp; &emsp;在实际应用中不需要弄明白这个方差代表什么，只需要满足上面这个式子即可。<br>&emsp; &emsp;如何去选择一个合适的k，可以尝试将k=1或其他初始值代入上面的式子计算进行比较然后再做调整。在实际操作中可以利用之前奇异值分解得到的$S$矩阵。<br><img src="https://img-blog.csdnimg.cn/5b06c62e8a384d18ae27ca04bfd89ef5.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="63压缩重现"><a href="#63压缩重现" class="headerlink" title="63压缩重现"></a>63压缩重现</h2><p>&emsp; &emsp;虽然我们之前对数据做了压缩，但是压缩后的低纬数据仍能表示原来高维数据所具有的信息。而且低维数据可以还原为高维数据，即将原来的式子反过来进行运算。<br><img src="https://img-blog.csdnimg.cn/0bfcdd7732d840ed9da6d8bc6a2e488e.png" alt="在这里插入图片描述"><br>&emsp; &emsp;这样我们计算出来的$x_{approx}$就很接近我们原来的x,这个过程我们称之为原始数据重构。<br><img src="https://img-blog.csdnimg.cn/f9a6be7359b841668d343335069e48b6.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="64应用PCA的建议"><a href="#64应用PCA的建议" class="headerlink" title="64应用PCA的建议"></a>64应用PCA的建议</h2><p>&emsp; &emsp;当我们在进行一个高纬度数据的有监督学习的时候，计算速度会很慢，此时就可以使用PCA对数据进行降维以达到加速的效果。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(x(1),y(1)),(x(2),y(2)),…,(x(m),y(m))</span><br><span class="line">Extract inputs:</span><br><span class="line">Unlabeled dataset: x(1),x(2),…,x(m)∈R10000</span><br><span class="line">↓PCA</span><br><span class="line">z(1),z(2),…,z(m)∈R1000</span><br><span class="line">New training <span class="built_in">set</span>:</span><br><span class="line">(z(1),y(1)),(z(2),y(2)),…,(z(m),y(m))</span><br></pre></td></tr></table></figure><p>&emsp; &emsp;使用PCA相对于建立了一个从x到z的映射，进行特征缩放。要注意的是在交叉验证集和测试集上要和训练集一样的PCA参数。因为我们保留了99%的方差，所以即使降维了也不会影响到模型分类的精度。<br><strong>PCA错误用法</strong><br>&emsp; &emsp;有一种对PCA的错误用法是使用PCA来防止过拟合。虽然PCA可以起到一些防止过拟合的作用，但这并不是其正确的用法，也不是防止过拟合的好方法，最好还是使用正则项。这是因为PCA虽然保留了99%的方差，但是仍会丢掉一些有用信息，而使用正则项因为实在有标签的数据上进行训练，所以不会有信息被丢掉。<br>&emsp; &emsp;还有一个对PCA的错误用法就是，在一开始设计机器学习系统的时候就讲PCA考虑进去，而没有想过不用PCA进行训练会有怎样的结果。正确的做法是当不使用PCA遇到问题（运行太慢、内存硬盘占用量大）的时候才考虑使用PCA。</p><hr><h1 id="异常检测"><a href="#异常检测" class="headerlink" title="异常检测"></a>异常检测</h1><h2 id="65-问题动机"><a href="#65-问题动机" class="headerlink" title="65 问题动机"></a>65 问题动机</h2><p>&emsp; &emsp;异常检测有点类似一个无监督学习<br><img src="https://img-blog.csdnimg.cn/718f8882ef6843d0bdf996274266081a.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>&emsp; &emsp;我们先对一些正常样本进行检测获取数据构建模型P，然后当一个新的样本进来的时候，如果它落在P的靠近中间的位置，那么可以判断它是一个正常的；如果落在远离中心的位置，那么就会被判定为是一个异常的样本。<br>&emsp; &emsp; 这种异常检测方法常被一些购物网站用来检测用户是否有异常行为，比如被盗号之类的。</p><h2 id="66高斯分布"><a href="#66高斯分布" class="headerlink" title="66高斯分布"></a>66高斯分布</h2><p><img src="https://img-blog.csdnimg.cn/5cd6f3b5f93445d598b45a3b933a1fbc.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>&emsp; &emsp;$\sigma$被称为标准差。<br><strong>参数估计</strong><br>&emsp; &emsp;参数估计即从数据集中估算出$\mu,\sigma$两个参数。<br>$\mu=\frac{1}{m}\sum^m_{i=1}x^{(i)},\sigma^2=\frac{1}{m}\sum^m_{i=1}(x^{(i)}-\mu)^2$<br>&emsp; &emsp;在一些统计课上学习的公式使用的是$\frac{1}{m-1},但在机器学习领域中更喜欢使用\frac{1}{m}$的版本，这对最终结果影响不大，因为机器学习使用的数据集一般都很大。</p><h2 id="67算法"><a href="#67算法" class="headerlink" title="67算法"></a>67算法</h2><p>&emsp; &emsp;为了得到一个最终的概率函数，我们会对每一个特征建立一个概率函数然后累乘<br>$$x_1 \backsim N(\mu_1,\sigma^2_1),x_2 \backsim N(\mu_2,\sigma^2_2),x_n \backsim N(\mu_n,\sigma^2_n)$$<br>$$P(x)=P(x_1;\mu_1,\sigma^2_1)P(x_2;\mu_2,\sigma^2_2)…P(x_n;\mu_n,\sigma^2_n)=\prod^n_{j=1}P(x_j;\mu_j,\sigma^2_j)$$<br>&emsp; &emsp;尽管从统计学角度说上面式子每一个特征都应该满足独立假设，但是在实际应用中即使独立假设不成立这个算法也能正常运行。<br><strong>异常检测算法</strong><br>&emsp; &emsp;1.Choose features $x_i$ that you think might be indicative of anomalous examples.<br>&emsp; &emsp;2.Fit parameters $μ_1,…,μ_n,σ^2_1,…,σ^2_n$<br>$$\mu_j=\frac{1}{m}\sum^m_{i=1}x_j^{(i)}$$<br>$$\sigma^2_j=\frac{1}{m}\sum^m_{i=1}(x^{(i)}<em>j-\mu_j)^2$$<br>&emsp; &emsp;3.Given new example $x$,compute $P(x)$:<br>$$P(x)=\prod^n</em>{j=1}P(x_j;\mu_j,\sigma^2_j)=\prod^n_{j=1}\frac{1}{\sqrt{2\pi}\sigma_j}exp(-\frac{(x_j-\mu_j)^2}{2\sigma^2_j})$$<br>&emsp; &emsp;Anomaly if $P(x)&lt;\varepsilon$</p><h2 id="68开发和评估异常检测系统"><a href="#68开发和评估异常检测系统" class="headerlink" title="68开发和评估异常检测系统"></a>68开发和评估异常检测系统</h2><p>&emsp; &emsp;虽然异常检测系统近似于一个无监督学习，但是在开发的过程中我们还是给他打上标签，正常样本为0，异常样本为1。然后将数据集划分为训练集、校验集和测试集，其中异常样本全部分配到校验集和测试集中。<br>&emsp; &emsp;然后使用无标签训练集来拟合目标函数$P(x)$.<br>&emsp; &emsp;在带标签的校验集和测试集上进行预测评估。因为正常样本会远大于异常样本产生样本偏斜，所以我们使用查准率与查全率、F1 Score的指标来进行评估。<br>&emsp; &emsp;同样使用交叉验证集来选择阈值参数$\varepsilon$</p><h2 id="69异常检测VS监督学习"><a href="#69异常检测VS监督学习" class="headerlink" title="69异常检测VS监督学习"></a>69异常检测VS监督学习</h2><p><strong>异常检测</strong><br>&emsp; &emsp;通常正常样本远大于异常样本，但仍能很好的工作。<br>&emsp; &emsp;通常遇到的异常情况都不尽相同，新的异常样本肯定和之前的截然不同，很难通过学习预测之后新异常的大概样子。当遇到一个新异常样本的时候，使用高斯函数会更快速的拟合。<br><strong>监督学习</strong><br>&emsp; &emsp;正负样本比例比较均匀。<br>&emsp; &emsp;通过对大量正负样本的学习能大致掌握新的正负样本可能的样子。</p><h2 id="70选择要使用的功能"><a href="#70选择要使用的功能" class="headerlink" title="70选择要使用的功能"></a>70选择要使用的功能</h2><p>&emsp; &emsp;如果你的数据不符合高斯分布，建议使用一个转换函数将其近似转换为高斯分布。<br><img src="https://img-blog.csdnimg.cn/cf861d8b596f48d68983cc67f5245deb.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>&emsp; &emsp;如果你现有的模型无法正常预测一个新的异常样本的时候，分析这个异常样本，从而提取出新的特征来进行模型训练，从而将其区分开来。</p><h2 id="71多变量高斯分布"><a href="#71多变量高斯分布" class="headerlink" title="71多变量高斯分布"></a>71多变量高斯分布</h2><p>&emsp; &emsp;参数$\mu,\Sigma(协方差矩阵)$<br>$$P(x;\mu,\Sigma)=\frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^\frac{1}{2}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))$$</p><h2 id="72使用多变量高斯分布的异常检测"><a href="#72使用多变量高斯分布的异常检测" class="headerlink" title="72使用多变量高斯分布的异常检测"></a>72使用多变量高斯分布的异常检测</h2><p> &emsp; &emsp;参数拟合：<br> &emsp; &emsp;&emsp; &emsp;训练集：$x^{(1)},x^{(2)},x^{(m)}$<br> $$\mu=\frac{1}{m}\sum^m_{i=1}x^{(i)},\Sigma=\frac{1}{m}\sum^m_{i=1}(x^{(i)}-\mu)(x^{(i)}-\mu)^T$$<br> &emsp; &emsp;原始的高斯分布模型可以算是多元高斯分布的一个特例，即Σ矩阵为对角阵的情况，它们都是与轴对其的。而多元高斯分布还能拟合出如对角线这样的一些更复杂的模型。<br><strong>原始模型VS多元高斯模型</strong><br> &emsp; &emsp;如果存在一个异常是由两个特征同时影响造成的，那么使用原始模型想要解决这个问题就要使用这两个特征组合起来创造新的特征来加入训练。相比之下多元高斯能自动捕获这两种特征之间的关系。<br>  &emsp; &emsp;原始模型的一大优势是计算成本低，它能适应大规模的数据集和特征。而多元高斯分布因为要计算协方差矩阵，计算量就要大得多。<br>   &emsp; &emsp;当训练样本很少的时候原始模型也能正常工作。而对于多元高斯分布，必须满足训练样本数量大于特征数量，不然协方差矩阵将是奇异矩阵不可逆，这种情况下就不能使用多元高斯分布。同样的，如果存在大量冗余特征也可能导致不可逆。一般我们只有在训练样本数量远大于特征数量的时候才使用多元高斯分布。</p><hr><h1 id="大规模机器学习"><a href="#大规模机器学习" class="headerlink" title="大规模机器学习"></a>大规模机器学习</h1><h2 id="73学习大数据集"><a href="#73学习大数据集" class="headerlink" title="73学习大数据集"></a>73学习大数据集</h2><p> &emsp; &emsp;在机器学习中，通常情况下决定因素往往不是最好的算法，而是谁的训练数据最多。</p><h2 id="74"><a href="#74" class="headerlink" title="74"></a>74</h2><p> &emsp; &emsp;每次梯度下降的时候都使用整个训练集的数据求梯度确实是收敛最快的方法，但是当数据集很大的时候相应的计算代价也会很大，这种使用全部数据集进行梯度下降的方法称为Batch梯度下降。<br>  &emsp; &emsp;随机梯度下降就是先把所有数据随机打乱，然后遍历对每一个数据进行梯度下降。随机梯度下降与Batch梯度下降不同的地方就在于不需要对多个数据进行求和，而是对每个数据单独梯度下降。虽然每次下降方向并不都是朝着最优的方向，但是总的趋势是收敛的，最终能得到一个近似于全局最小的结果。<br>  <img src="https://img-blog.csdnimg.cn/8e008e6ad7524aed9c97adeebe84d412.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"></p><h2 id="75Mini-Batch梯度下降"><a href="#75Mini-Batch梯度下降" class="headerlink" title="75Mini-Batch梯度下降"></a>75Mini-Batch梯度下降</h2><p>&emsp; &emsp;Mini-Batch梯度下降介于Batch梯度下降和随机梯度下降之间，他每次迭代使用b个样本。b的常用取值在2～100之间。<br>&emsp; &emsp;Mini-Batch梯度下降比随机梯度下降要好的一点是，它可以在b个样本中实现并行计算，而不是遍历每一个样本。</p><h2 id="76随机梯度下降收敛"><a href="#76随机梯度下降收敛" class="headerlink" title="76随机梯度下降收敛"></a>76随机梯度下降收敛</h2><p>&emsp; &emsp;在进行Batch梯度下降时，我们每进行一次迭代就计算一次cost，以此绘图观察是否收敛。<br>&emsp; &emsp;在随机梯度下降中，可以每进行n次迭代就计算前n次迭代的平均cost，然后绘图观察是否收敛。<br>&emsp; &emsp;1）因为并不是每次都走向全局最优，所以cost曲线会以一个震荡的趋势逐渐下降。如果减小学习率的话则会下降的更慢震荡幅度更小，但是最终会更接近于全局最优。<br>&emsp; &emsp;2）如果提高n的取值，曲线将会变得更为平滑，但是这样信息就会有所延迟。<br>&emsp; &emsp;3）如果看到曲线一直震荡没有明显下降的趋势，可增大n值再观察，此时会发现函数趋势实际上是减小的。如果n太小，因为有噪声的存在会不好观察。如果增大n值观察仍没有下降，那么可能算法就存在问题。<br>&emsp; &emsp;4）如果观察到曲线是上升的趋势，那就说明是发散的，应该降低学习率。<br><img src="https://img-blog.csdnimg.cn/143571d41a454bea9117ec9884c163c3.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>&emsp; &emsp;应为随机梯度下降随着接近全局最小，会在周围不停震荡很难接近。要想使最终结果更接近最优值，可以让学习率随着时间减小，减小震荡，比如：<br>$$\alpha=\frac{const1}{iterationNumber+const2}$$</p><h2 id="77在线学习"><a href="#77在线学习" class="headerlink" title="77在线学习"></a>77在线学习</h2><p>&emsp; &emsp;在线学习就是每当用户产生新的数据的时候就进行学习，然后将该数据扔掉不再使用，这样每次只学习一个数据。因为一般的大型网站会产生源源不断的数据流，所以没必要多次使用一个样本，如果用户量较少就不建议使用在线学习。<br>&emsp; &emsp;这种在线学习模型能够随着时间变化、经济环境变化去适应用户的喜好。</p><h2 id="78减少映射与数据并行"><a href="#78减少映射与数据并行" class="headerlink" title="78减少映射与数据并行"></a>78减少映射与数据并行</h2><p><strong>Map-Reduce</strong><br>&emsp; &emsp;Map-Reduce的思想是把数据集分成多份，然后分别在不同的机器上跑。然后在将每个机器上计算出来的结果由中央服务器汇总再进行梯度下降计算。<br><img src="https://img-blog.csdnimg.cn/38047a649e9b4393965ec0a3a287469b.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAcXFfNDA1NDExMTE=,size_20,color_FFFFFF,t_70,g_se,x_16" alt="在这里插入图片描述"><br>&emsp; &emsp;要将Map-Reduce应用在你的机器学习模型上，你需要考虑的一个关键问题是你的学习算法能否表示成对训练集的一种求和。实际上很多机器算法模型都能表示成。<br>&emsp; &emsp;同样的Map-Reduce方法也能应用在多CPU和多核计算机上。</p>]]></content>
    
    
    <summary type="html">刚接触机器学习发现有些东西真的好神奇啊</summary>
    
    
    
    <category term="机器学习" scheme="http://example.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="机器学习" scheme="http://example.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    <category term="吴恩达" scheme="http://example.com/tags/%E5%90%B4%E6%81%A9%E8%BE%BE/"/>
    
  </entry>
  
  <entry>
    <title>外地上学感悟</title>
    <link href="http://example.com/2021/08/24/%E5%A4%96%E5%9C%B0%E4%B8%8A%E5%AD%A6%E6%84%9F%E6%82%9F/"/>
    <id>http://example.com/2021/08/24/%E5%A4%96%E5%9C%B0%E4%B8%8A%E5%AD%A6%E6%84%9F%E6%82%9F/</id>
    <published>2021-08-24T09:10:26.000Z</published>
    <updated>2021-08-31T01:12:58.584Z</updated>
    
    <content type="html"><![CDATA[<h1 id="山东"><a href="#山东" class="headerlink" title="山东"></a>山东</h1><p>刚出飞机场的时候，我感觉很舒服，可能是因为四川太热了吧。这边的天气让我倍感舒服，甚至感觉到处都是鸟语花香，深吸一口气都能闻到不远处海风的味道(其实胶东机场离海边还是有点距离的，完全是我的心理作用)。机场有个大巴是直达学校的，这点让我倍感欣慰，省去了地铁转线的麻烦和打车费用的昂贵</p><h1 id="学校"><a href="#学校" class="headerlink" title="学校"></a>学校</h1><p>大巴停在学校对面的马路上，一车的校友都在等着红绿灯准备拥入学校的怀抱。学校真的很热情，只要出去必定会关心你是不是新生，然后向你推销电话卡。住宿条件没我想象中的好，但是也没那么差，是博士楼改造的，一厅三室一卫一浴，每个房间住两人，总共来了三个(包括我)，还剩三个本省的没来(因为疫情，本省和外省分开返校)。只是学校到处都是坡，走起来挺费劲的。</p><h1 id="饮食"><a href="#饮食" class="headerlink" title="饮食"></a>饮食</h1><p>我感觉山东这边的人好像都吃的比较甜，什么菜也感觉不到辣味。还好我胃口不挑，也没太多不习惯。</p><h1 id="气候"><a href="#气候" class="headerlink" title="气候"></a>气候</h1><p>好像是热天过去了吧，虽然没有空调，但晚上睡得还是挺舒服的，不冷不热的。但有时候湿度真的很大，很怕衣服晒不干，有时候太阳有很大，感觉紫外线很强。奇奇怪怪的天气，打伞也不奇怪。</p><h1 id="实验室"><a href="#实验室" class="headerlink" title="实验室"></a>实验室</h1><p>很庆幸能加入这个实验室，感觉实验室氛围挺好的（老师不在的情况下）。没有内卷，也没有人心，大家都聊得挺好的，好像师兄们能力都挺强的。但这实验室工作强度好像挺高的，希望自己能抗住压力。</p><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>明天本地新生就要开始返校了，我那剩下的三个室友就要来了，还有实验室的新生也要来了(这届的新生除了我都是本省的。。。。。)</p>]]></content>
    
    
    <summary type="html">距离来到山东已经有2天了，还是头一次跑这么远，距离老家有2098.2公里，心中万分感慨啊</summary>
    
    
    
    <category term="生活" scheme="http://example.com/categories/%E7%94%9F%E6%B4%BB/"/>
    
    
    <category term="读研" scheme="http://example.com/tags/%E8%AF%BB%E7%A0%94/"/>
    
  </entry>
  
  <entry>
    <title>第一篇博客</title>
    <link href="http://example.com/2021/08/03/welcome-to-hexo/"/>
    <id>http://example.com/2021/08/03/welcome-to-hexo/</id>
    <published>2021-08-03T03:55:48.000Z</published>
    <updated>2021-08-31T01:12:41.123Z</updated>
    
    <content type="html"><![CDATA[<h1 id="这是我在hexo写的第一篇博客"><a href="#这是我在hexo写的第一篇博客" class="headerlink" title="这是我在hexo写的第一篇博客"></a>这是我在hexo写的第一篇博客</h1><p>最近突然就想着搭建一个属于自己的博客，想要一个自己的小天地用来记录自己的研究生生活。<br>这个博客是用hexo+next来搭建完成的，很喜欢这种简约的风格。我是参作<a href="http://yearito.cn/">yearito</a>来搭建的，基本上可以说是他的复制了。</p>]]></content>
    
    
    <summary type="html">关于搭建这个博客</summary>
    
    
    
    <category term="hexo" scheme="http://example.com/categories/hexo/"/>
    
    
    <category term="hexo" scheme="http://example.com/tags/hexo/"/>
    
  </entry>
  
</feed>
